{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "$$\\mathbf{o}=\\mathbf{W}\\mathbf{x}+\\mathbf{b}$$\n",
    "$$\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o})\\quad \\text{where}\\quad \\hat{y}_j = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "Here we still want to maximize the likelihood of $P(\\mathbf{Y}|\\mathbf{X})$, which is equivalent to minimizing the negtive log-likelihood\n",
    "$$-\\log P(\\mathbf{Y} \\mid \\mathbf{X}) = \\sum_{i=1}^n -\\log P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)})\n",
    "= \\sum_{i=1}^n l(\\mathbf{y}^{(i)}, \\hat{\\mathbf{y}}^{(i)}),$$\n",
    "    And for each pair of $<\\mathbf{y}, \\mathbf{\\hat y}>$ over $q$ classes, the negtive log-likelihood is\n",
    "$$l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{j=1}^q y_j \\log \\hat{y}_j.$$\n",
    "\n",
    "So the total loss function is \n",
    "$$L(\\mathbf{Y}, \\mathbf{\\hat Y})=-\\frac{1}{n}\\sum_{i=1}^n\\sum_{j=1}^q y_j \\log \\hat{y}_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implemantation from scratch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the dataset\n",
    "Here we are using the Fashion-MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a transformer to transfer the PIL.Image type into a torch.FloatTensor type, and the range of the tensor is [0, 1.0]\n",
    "trans = torchvision.transforms.ToTensor()\n",
    "\n",
    "mnist_train = torchvision.datasets.FashionMNIST(root=\"./data\", train=True, transform=trans, download=True)\n",
    "mnist_test = torchvision.datasets.FashionMNIST(root=\"./data\", train=False, transform=trans, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    }
   ],
   "source": [
    "print(mnist_train)\n",
    "print(mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mnist_train` object and `mnist_test` objects are both `torch.utils.data.Dataset` objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True\n"
     ]
    }
   ],
   "source": [
    "print(isinstance(mnist_train, torch.utils.data.Dataset),isinstance(mnist_test, torch.utils.data.Dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can access them through index(key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(mnist_train[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve text labels\n",
    "Here we define a function to retrieve the corresponding text label given the numerical label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fashion_mnist_labels(labels):\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt',\n",
    "        'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int(i)] for i in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read a minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_iter = iter(torch.utils.data.DataLoader(mnist_train, batch_size = batch_size, shuffle=True, num_workers=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put all things together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_fashion_mnist(batch_size, resize=None):  #@save\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n",
    "    trans = [torchvision.transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, torchvision.transforms.Resize(resize))\n",
    "    trans = torchvision.transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=\"./data\",\n",
    "                                                    train=True,\n",
    "                                                    transform=trans,\n",
    "                                                    download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=\"./data\",\n",
    "                                                   train=False,\n",
    "                                                   transform=trans,\n",
    "                                                   download=True)\n",
    "    return (torch.utils.data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                            num_workers=4),\n",
    "            torch.utils.data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "                            num_workers=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "W = torch.normal(0,0.01, size=(num_inputs,num_outputs), requires_grad=True)\n",
    "b = torch.zeros(num_outputs, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the softmax operation\n",
    "The softmax function is:\n",
    "$$\\mathrm{softmax}(\\mathbf{X})_{ij} = \\frac{\\exp(\\mathbf{X}_{ij})}{\\sum_k \\exp(\\mathbf{X}_{ik})}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = torch.exp(X)\n",
    "    partition = torch.sum(X_exp, axis=1, keepdim=True)\n",
    "    return X_exp/partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_rg(X):\n",
    "    return softmax(torch.matmul(X.reshape(-1, W.shape[0]),W)+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cross-entropy loss function\n",
    "Recall that the cross entropy loss is defined as:\n",
    "$$l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{j=1}^q y_j \\log \\hat{y}_j.$$\n",
    "\n",
    "And among the q classes, there is only one $y_j$ bigger than 0. So we just need to pick the corresponding ${\\hat y}_j$ for which $j$ we have $y_j>0$. And multiplying them to get the final cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this function calculates the cross-entropy loss for each pair of <y_hat,y>\n",
    "def cross_entropy(y_hat, y):\n",
    "    return -torch.log(y_hat[range(len(y)), y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.397758, accuracy 84.000000\n",
      "epoch 2, loss 0.311017, accuracy 86.000000\n",
      "epoch 3, loss 0.404468, accuracy 83.000000\n",
      "epoch 4, loss 0.365813, accuracy 85.000000\n",
      "epoch 5, loss 0.514253, accuracy 78.000000\n",
      "epoch 6, loss 0.307236, accuracy 85.000000\n",
      "epoch 7, loss 0.397973, accuracy 86.000000\n",
      "epoch 8, loss 0.397311, accuracy 81.000000\n",
      "epoch 9, loss 0.398390, accuracy 82.000000\n",
      "epoch 10, loss 0.439022, accuracy 83.000000\n"
     ]
    }
   ],
   "source": [
    "num_epochs=10\n",
    "lr = 0.1\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=None)\n",
    "def sgd(params, lr, batch_size):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr*param.grad/batch_size\n",
    "            param.grad.zero_()\n",
    "            \n",
    "for epoch in range(num_epochs):\n",
    "    for X,y in train_iter:\n",
    "        y_hat = soft_rg(X)\n",
    "        ce_loss = cross_entropy(y_hat,y)\n",
    "        ce_loss.sum().backward()\n",
    "        sgd([W,b],lr,batch_size)\n",
    "    with torch.no_grad():\n",
    "        y_hat = soft_rg(X)\n",
    "        ce_loss = cross_entropy(y_hat, y)\n",
    "        acct = accuracy(y_hat, y\n",
    "        print(f'epoch {epoch + 1}, loss {float(ce_loss.mean()):f}, accuracy {float(acct):f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concise Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten()\n",
       "  (1): Linear(in_features=784, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_reg = torch.nn.Sequential(torch.nn.Flatten(), torch.nn.Linear(num_inputs, num_outputs))\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m)==torch.nn.Linear:\n",
    "        m.weight.data.normal_(0,0.01)\n",
    "\n",
    "soft_reg.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(soft_reg.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.439022, accuracy 79.000000\n",
      "epoch 2, loss 0.439022, accuracy 85.000000\n",
      "epoch 3, loss 0.439022, accuracy 81.000000\n",
      "epoch 4, loss 0.439022, accuracy 79.000000\n",
      "epoch 5, loss 0.439022, accuracy 81.000000\n",
      "epoch 6, loss 0.439022, accuracy 78.000000\n",
      "epoch 7, loss 0.439022, accuracy 85.000000\n",
      "epoch 8, loss 0.439022, accuracy 77.000000\n",
      "epoch 9, loss 0.439022, accuracy 76.000000\n",
      "epoch 10, loss 0.439022, accuracy 91.000000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X,y in train_iter:\n",
    "        l = loss(soft_reg(X), y)\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    with torch.no_grad():\n",
    "        y_hat = soft_reg(X)\n",
    "        l = loss(y_hat, y)\n",
    "        acct = accuracy(y_hat, y)\n",
    "        print(f'epoch {epoch + 1}, loss {float(ce_loss.mean()):f}, accuracy {float(acct):f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
