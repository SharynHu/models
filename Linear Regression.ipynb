{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imprementation from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic dataset\n",
    "The input $\\mathbf{x}=[x_1, x_2]$ of which both elements are from the normal distribution $\\mathcal{N}(0,1)$.\n",
    "The output $y$ is calculated from\n",
    "   \n",
    "   $$y=\\mathbf{w}^T \\mathbf{x}+b+\\epsilon$$\n",
    "\n",
    "where $\\epsilon$ is a Gaussian noise driven from $\\mathcal{N}(0,0.01)$, $\\mathbf{w}=\\begin{bmatrix}2 & -3.4 \\end{bmatrix}^T$, and $b=4.2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_data(w, b, num_examples):\n",
    "    X = torch.normal(mean=0, std=1, size=(num_examples, len(w)))\n",
    "    y = torch.matmul(X, w)+b\n",
    "    epsilon = torch.normal(0, 0.01, size=y.shape)\n",
    "    y += epsilon\n",
    "    return X, y.reshape([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = torch.tensor([2,-3.4])\n",
    "true_b = torch.tensor(4.2)\n",
    "features, labels = synthesize_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the dataset\n",
    "\n",
    "Here we define a generator that will return us a minibatch of the training set each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    \n",
    "    random.shuffle(indices)\n",
    "\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(indices[i:min(i+\n",
    "                                                   batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter initialization\n",
    "By default, the `requires_grad`attribiute for a tensor is `False`. So we need to explicitly specify them when we initialize $\\mathbf{w}$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.normal(0,0.01, size = (2,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "The model is defined by:\n",
    "$$y = \\mathbf{w}^T\\mathbf{x}+b$$\n",
    "With a minibatch of inputs, $\\mathbf{X}$ is an matrix, so it can be written as:\n",
    "$$\\mathbf{y}=\\mathbf{X}\\mathbf{w}+b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X, w, b):\n",
    "    return torch.mm(X, w)+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def squared_loss(y_hat, y):\n",
    "#     return (y_hat-y.reshape(y_hat.shape))**2/2\n",
    "def squared_loss(y_hat, y):  #@save\n",
    "    \"\"\"Squared loss.\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape))**2 / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "The updating rule for the parameters are\n",
    "$$\\mathbf{w} = \\mathbf{w}-\\eta * \\mathbf{w}.grad$$\n",
    "$$b = b-\\eta * b.grad$$\n",
    "\n",
    "Since pytorch will accululate gradients, we need to set the variable manually to zero after we've done a computation. For why Pytorh is doing so plz check this link [Why do we need to set the gradients manually to zero in pytorch?\n",
    "](https://discuss.pytorch.org/t/why-do-we-need-to-set-the-gradients-manually-to-zero-in-pytorch/4903)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr*param.grad/batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "The whole process of the traning is:\n",
    "1. initialize parameters;\n",
    "2. compute loss function value based on the current parameters; \n",
    "3. compute the gradients;\n",
    "4. update paramters acorrding to the gradients; check if we meet the stopping criteria; if not, go to step 2, else, go to step 5;\n",
    "5. return the paramters;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.022369\n",
      "epoch 2, loss 0.000133\n",
      "epoch 3, loss 0.000071\n"
     ]
    }
   ],
   "source": [
    "lr = 0.03\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        y_hat = linreg(X, w, b)\n",
    "        sq_loss = squared_loss(y_hat, y)\n",
    "        sq_loss.sum().backward()\n",
    "        sgd([w, b], lr, batch_size)\n",
    "    # after each iteration, print out the traning loss\n",
    "    with torch.no_grad():\n",
    "        y_hat = linreg(X, w, b)\n",
    "        sq_loss = squared_loss(y_hat, y)\n",
    "        print(f'epoch {epoch + 1}, loss {float(sq_loss.mean()):f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in estimating w: tensor([ 0.0006, -0.0002], grad_fn=<SubBackward0>)\n",
      "error in estimating b: tensor([0.0008], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'error in estimating w: {true_w - w.reshape(true_w.shape)}')\n",
    "print(f'error in estimating b: {true_b - b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Dataset\n",
    "The synthetic dataset we generated here is the same as the one we generated before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = torch.tensor([2,-3.4])\n",
    "true_b = torch.tensor(4.2)\n",
    "features, labels = synthesize_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Iterator\n",
    "Here we use `torch.utils.data.TensorDataset` to wrap our synthetic dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load__array(data_arrays, batch_size, is_train=True):\n",
    "    dataset = torch.utils.data.TensorDataset(*data_arrays)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size = batch_size, shuffle=is_train)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "The model is the same.\n",
    "The model is defined by:\n",
    "$$y = \\mathbf{w}^T\\mathbf{x}+b$$\n",
    "With a minibatch of inputs, $\\mathbf{X}$ is an matrix, so it can be written as:\n",
    "$$\\mathbf{y}=\\mathbf{X}\\mathbf{w}+b$$\n",
    "\n",
    "This time we are using the `nn.Sequential` and `nn.Linear`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = torch.nn.Sequential(torch.nn.Linear(2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Parameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lin_reg.weight.data.normal_(0, 0.01)\n",
    "lin_reg.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(lin_reg.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss 0.000270\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000098\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "num_epochs = 3\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    for X, y in load__array([features, labels], batch_size, is_train=True):\n",
    "        l = loss(lin_reg(X), y)\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    l = loss(lin_reg(features), labels)\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in estimating w: tensor([-0.0005,  0.0004])\n",
      "error in estimating b: tensor([-0.0002])\n"
     ]
    }
   ],
   "source": [
    "w = lin_reg[0].weight.data\n",
    "print('error in estimating w:', true_w - w.reshape(true_w.shape))\n",
    "b = lin_reg[0].bias.data\n",
    "print('error in estimating b:', true_b - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
